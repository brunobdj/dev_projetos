{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from typing import Any\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-537DDEC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>teste_pratico</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2472c567d10>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName('teste_pratico')\\\n",
    ".config('spark.master', 'local[4]')\\\n",
    ".config('spark.shuffle.sql.partitions', 2)\\\n",
    ".getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Manipulação de Dados\n",
    "### Criação de DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Data Scientist\"),\n",
    "    (\"Bob\", 45, \"Data Engineer\"),\n",
    "    (\"Cathy\", 29, \"Data Analyst\"),\n",
    "    (\"David\", 35, \"Data Scientist\")\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "\n",
    "# Criação do DataFrame em Spark\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de DataFrame (Age>30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age30Plus = df.select(\"Name\", \"Age\").filter(df[\"Age\"] > 30)\n",
    "df_age30Plus.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamento e Ordenação de DataFrame (Age: Médias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|    Occupation|Average Age|\n",
      "+--------------+-----------+\n",
      "| Data Engineer|       45.0|\n",
      "|Data Scientist|       34.5|\n",
      "|  Data Analyst|       29.0|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_occMean = df.groupBy(\"Occupation\").agg(F.avg(\"Age\").alias(\"Average Age\")).orderBy(\"Average Age\", ascending=False)\n",
    "df_occMean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Funções Avançadas\n",
    "### Uso de UDFs (User Defined Functions)\n",
    "#### Categorização das idades usando udfs (Jovem; Adulto; Senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+------------+\n",
      "| Name|Age|    Occupation|Age Category|\n",
      "+-----+---+--------------+------------+\n",
      "|Alice| 34|Data Scientist|      Adulto|\n",
      "|  Bob| 45| Data Engineer|      Senior|\n",
      "|Cathy| 29|  Data Analyst|       Jovem|\n",
      "|David| 35|Data Scientist|      Adulto|\n",
      "+-----+---+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def catAge(age):\n",
    "    if age < 30:\n",
    "        return \"Jovem\"\n",
    "    elif 30 <= age <= 40:\n",
    "        return \"Adulto\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "\n",
    "catAge_udf = F.udf(catAge, StringType())\n",
    "\n",
    "df_catAge = df.withColumn(\"Age Category\", catAge_udf(df[\"Age\"]))\n",
    "df_catAge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionando a diferença de idades por individuos, usando a function Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+-----------+--------+\n",
      "| Name|Age|    Occupation|Average Age|Age Diff|\n",
      "+-----+---+--------------+-----------+--------+\n",
      "|Cathy| 29|  Data Analyst|       29.0|     0.0|\n",
      "|  Bob| 45| Data Engineer|       45.0|     0.0|\n",
      "|Alice| 34|Data Scientist|       34.5|    -0.5|\n",
      "|David| 35|Data Scientist|       34.5|     0.5|\n",
      "+-----+---+--------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "winOcc = Window.partitionBy(\"Occupation\")\n",
    "\n",
    "df_winOcc_avg = df.withColumn(\"Average Age\", F.avg(\"Age\").over(winOcc))\n",
    "\n",
    "# Cálculo da diferença entre a idade individual e a média\n",
    "df_ageDiff = df_winOcc_avg.withColumn(\"Age Diff\", F.col(\"Age\") - F.col(\"Average Age\"))\n",
    "df_ageDiff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Performance e Otimização\n",
    "### Particionamento\n",
    "\n",
    "#### O particionamento melhora a performance ao distribuir os dados em diferentes partições, o que permite que operações de leitura e escrita sejam executadas em paralelo essa performance é escalavel quando ultrapassamos esse exemplo e deparamos com um cenário de big data, onde o custo de fazer um filtro simples aumenta o recurso computacional disposto. Segue um exemplo de partição para os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part = df.repartition(4, \"Occupation\")\n",
    "df_part.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O broadcast se beneficia da performance, ao atuarmos com sistemas distribuidos onde é uma técnica de otimização onde um DataFrame pequeno é enviado para todos os nós de um cluster, permitindo que o join seja realizado localmente em cada nó, diminuindo o fluxo de dados por nós, assim reduzindo a movimentação de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---+------+\n",
      "|    Occupation| Name|Age|Salary|\n",
      "+--------------+-----+---+------+\n",
      "|Data Scientist|Alice| 34|100000|\n",
      "| Data Engineer|  Bob| 45| 95000|\n",
      "|Data Scientist|David| 35|100000|\n",
      "+--------------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_broadCast = spark.createDataFrame([(\"Data Scientist\", 100000), (\"Data Engineer\", 95000)], [\"Occupation\", \"Salary\"])\n",
    "\n",
    "\n",
    "# broadCast Join usando o `Occupation` como indexador \n",
    "result_df = df.join(F.broadcast(df_broadCast), \"Occupation\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Integração com Outras Tecnologias\n",
    "### Leitura e Escrita de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------+\n",
      "| Name|Age|    Occupation|\n",
      "+-----+---+--------------+\n",
      "|Alice| 34|Data Scientist|\n",
      "|  Bob| 45| Data Engineer|\n",
      "|Cathy| 29|  Data Analyst|\n",
      "|David| 35|Data Scientist|\n",
      "+-----+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o exemplo vou utilizar o própio df que criei no começo do teste \n",
    "df2= pd.DataFrame(data, columns=columns)\n",
    "df2.to_csv(\"data/dim_colaborador.csv\", index=False)\n",
    "\n",
    "# Leitura de CSV\n",
    "df_csv = spark.read.csv(\"data/dim_colaborador.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Escrita em Parquet\n",
    "# df_csv.write.mode(\"overwrite\").save(\"dim_colaborador.parquet\")\n",
    "# df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para exemplo de interação com o HDFS seguem o script em questão onde retiro do Escrevo uma tabela de exemplo no hadoop\n",
    "\n",
    "```{python3}\n",
    "spark.conf.set(\"fs.defaultFS\", \"hdfs://namenode:8020\")\n",
    "# Leitura \n",
    "df_hdfs = spark.read.csv(\"hdfs://namenode:8020/input/path/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filtro\n",
    "df_filtered = df_hdfs.filter(df_hdfs[\"column\"] > 100)\n",
    "\n",
    "# Escrita \n",
    "df_filtered.write.csv(\"hdfs://namenode:8020/output/path/file.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "spark.conf.set(\"fs.defaultFS\", \"hdfs://namenode:8020\")\n",
    "# Leitura \n",
    "df_hdfs = spark.read.csv(\"hdfs://namenode:8020/input/path/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filtro\n",
    "df_filtered = df_hdfs.filter(df_hdfs[\"column\"] > 100)\n",
    "\n",
    "# Escrita \n",
    "df_filtered.write.csv(\"hdfs://namenode:8020/output/path/file.csv\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Problema de Caso\n",
    "### Processamento de Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrando a leitura do Log solicitado com exemplos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Demonstrando Leitura de log \n",
    "df_logs = spark.read.csv(\"hdfs://namenode:8020/data/log.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Contagem por usuário\n",
    "df_userId = df_logs.groupBy(\"user_id\").count()\n",
    "\n",
    "# Identificação dos 10 usuários mais ativos\n",
    "df_userId10 = df_userId.orderBy(\"count\", ascending=False).limit(10)\n",
    "\n",
    "# Salvamento do resultado em um arquivo CSV\n",
    "df_userId10.write.csv(\"hdfs://namenode:8020/data/df_userId10.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obs: \n",
    "## A partir da interação com o hadoop ultilizei o cluster no ambiente databricks com as minhas credenciais  (não serão disponibilizadas), mas as configurações foram essas: \n",
    "```{python}\n",
    "spark.conf.set(\"fs.azure.account.key.<your-storage-account-name>.blob.core.windows.net\", \"<your-access-key>\")\n",
    "\n",
    "df_hdfs = spark.read.csv(\"wasbs://<your-container-name>@<your-storage-account-name>.blob.core.windows.net/teste.csv\", header=True, inferSchema=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
